{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73c0b63f",
      "metadata": {
        "editable": true,
        "id": "73c0b63f"
      },
      "source": [
        "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
        "doconce format html exercisesweek47.do.txt  -->\n",
        "<!-- dom:TITLE: Exercise week 47 -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb10d9d",
      "metadata": {
        "editable": true,
        "id": "8cb10d9d"
      },
      "source": [
        "# Exercise week 47\n",
        "**November 18-22, 2024**\n",
        "\n",
        "Date: **Deadline is Friday November 22 at midnight**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b90add8",
      "metadata": {
        "editable": true,
        "id": "2b90add8"
      },
      "source": [
        "# Overarching aims of the exercises this week\n",
        "\n",
        "The exercise set this week is meant as a summary of many of the\n",
        "central elements in various machine learning algorithms, with a slight\n",
        "bias towards deep learning methods and their training. You don't need to answer all questions.\n",
        "\n",
        "The last weekly exercise (week 48) is a general course survey."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f3ae78",
      "metadata": {
        "editable": true,
        "id": "b4f3ae78"
      },
      "source": [
        "## Exercise 1: Linear and logistic regression methods\n",
        "\n",
        "1. What is the main difference between ordinary least squares and Ridge regression?\n",
        "\n",
        "2. Which kind of data set would you use logistic regression for?\n",
        "\n",
        "3. In linear regression you assume that your output is described by a continuous non-stochastic function $f(x)$. Which is the equivalent function in logistic regression?\n",
        "\n",
        "4. Can you find an analytic solution to a logistic regression type of problem?\n",
        "\n",
        "5. What kind of cost function would you use in logistic regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "- The main difference lies in **regularization**. Ridge regression includes a penalty term to the loss function that discourages large coefficients:\n",
        "\n",
        "  $$\n",
        "  \\text{Ridge Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
        "  $$\n",
        "\n",
        "  where $\\lambda$ controls the strength of regularization. Ordinary least squares (OLS) does not include this penalty term and only minimizes the residual sum of squares.\n",
        "\n",
        "- Logistic regression is used for datasets with a **binary classification problem**, where the target variable $y$ has two classes (e.g., $y \\in \\{0, 1\\}$).\n",
        "\n",
        "- In logistic regression, the equivalent function is the **logistic (sigmoid) function**:\n",
        "\n",
        "  $$\n",
        "  f(x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "\n",
        "  where $z = \\mathbf{w}^T \\mathbf{x} + b$.\n",
        "\n",
        "- No, there is no **closed-form analytic solution** for logistic regression because the cost function is non-linear. Optimization is performed using iterative methods like **gradient descent**.\n",
        "\n",
        "- The cost function used in logistic regression is the **log-loss** (cross-entropy loss):\n",
        "\n",
        "  $$\n",
        "  J(\\mathbf{w}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
        "  $$\n",
        "\n",
        "  where $\\hat{y}_i$ is the predicted probability.\n"
      ],
      "metadata": {
        "id": "dmbQBvp8oBod"
      },
      "id": "dmbQBvp8oBod"
    },
    {
      "cell_type": "markdown",
      "id": "755cfd27",
      "metadata": {
        "editable": true,
        "id": "755cfd27"
      },
      "source": [
        "## Exercise 2: Deep learning\n",
        "\n",
        "1. What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?\n",
        "\n",
        "2. Describe the architecture of a typical feed forward  Neural Network (NN).\n",
        "\n",
        "3. You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isnâ€™t good. What can you do to reduce overfitting?\n",
        "\n",
        "4. How would you know if your model is suffering from the problem of exploding Gradients?\n",
        "\n",
        "5. Can you name and explain a few hyperparameters used for training a neural network?\n",
        "\n",
        "6. Describe the architecture of a typical Convolutional Neural Network (CNN)\n",
        "\n",
        "7. What is the vanishing gradient problem in Neural Networks and how to fix it?\n",
        "\n",
        "8. When it comes to training an artificial neural network, what could the reason be for why the cost/loss doesn't decrease in a few epochs?\n",
        "\n",
        "9. How does L1/L2 regularization affect a neural network?\n",
        "\n",
        "10. What is(are) the advantage(s) of deep learning over traditional methods like linear regression or logistic regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "\n",
        "- An **activation function** introduces non-linearity into a neural network, enabling it to learn complex patterns in the data. Without activation functions, the model would act as a linear mapping, regardless of depth.\n",
        "\n",
        "  Three common activation functions are:\n",
        "  - **ReLU (Rectified Linear Unit)**:\n",
        "  $$\n",
        "  f(x) = \\max(0, x)\n",
        "  $$\n",
        "  Pros: Computationally efficient and mitigates the vanishing gradient problem.  \n",
        "  Cons: Suffers from the \"dying ReLU\" problem (neurons output zero permanently).\n",
        "\n",
        "  - **Sigmoid**:\n",
        "  $$\n",
        "  f(x) = \\frac{1}{1 + e^{-x}}\n",
        "  $$\n",
        "  Pros: Outputs probabilities in the range (0, 1), suitable for binary classification.  \n",
        "  Cons: Vanishing gradients for large input values.\n",
        "\n",
        "  - **Tanh**:\n",
        "  $$\n",
        "  f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "  $$\n",
        "  Pros: Outputs in the range (-1, 1), better for zero-centered data than Sigmoid.  \n",
        "  Cons: Also prone to vanishing gradients.\n",
        "\n",
        "- A **feedforward neural network (NN)** has:\n",
        "  - **Input Layer**: Receives input features.\n",
        "  - **Hidden Layers**: Composed of neurons performing weighted sums followed by activation functions.\n",
        "  - **Output Layer**: Produces predictions, e.g., Softmax for classification or linear for regression.\n",
        "\n",
        "  Information flows forward, and weights are updated during backpropagation.\n",
        "\n",
        "- To reduce overfitting in deep neural networks:\n",
        "  - **Regularization**: Apply L1 or L2 penalties to constrain large weights.\n",
        "  - **Dropout**: Randomly deactivate neurons during training.\n",
        "  - **Early Stopping**: Halt training when validation loss stops improving.\n",
        "  - **Data Augmentation**: Create additional training samples using transformations.\n",
        "  - **Increase Dataset Size**: Collect more samples.\n",
        "  - **Simplify Model**: Reduce the number of layers or neurons.\n",
        "\n",
        "- A model suffers from **exploding gradients** if:\n",
        "  - Gradients have excessively high magnitudes.\n",
        "  - Loss fluctuates or outputs `NaN`.\n",
        "  - Weight values grow uncontrollably.\n",
        "\n",
        "- Common hyperparameters in neural networks:\n",
        "  - **Learning Rate**: Controls the step size for weight updates.\n",
        "  - **Batch Size**: Determines the number of samples used per training step.\n",
        "  - **Number of Epochs**: Total passes over the training dataset.\n",
        "  - **Dropout Rate**: Fraction of neurons to deactivate during training.\n",
        "  - **Optimizer**: Algorithm for weight updates, e.g., SGD or Adam.\n",
        "\n",
        "- A **Convolutional Neural Network (CNN)** consists of:\n",
        "  - **Convolutional Layers**: Extract features using filters.\n",
        "  - **Activation Functions**: Typically ReLU.\n",
        "  - **Pooling Layers**: Downsample dimensions (e.g., MaxPooling).\n",
        "  - **Fully Connected Layers**: Map features to output space.\n",
        "  - **Output Layer**: Produces predictions, such as probabilities (Softmax).\n",
        "\n",
        "- The **vanishing gradient problem** occurs when gradients become too small during backpropagation, leading to negligible updates in earlier layers.\n",
        "\n",
        "  Fixes include:\n",
        "  - Using ReLU activation.\n",
        "  - Proper weight initialization.\n",
        "  - Employing architectures with skip connections.\n",
        "\n",
        "- Reasons for the cost/loss not decreasing in a few epochs:\n",
        "  - **Learning Rate Too High/Low**: Causes poor convergence.\n",
        "  - **Improper Weight Initialization**: Leads to unstable gradients.\n",
        "  - **Insufficient Model Capacity**: Model unable to fit data complexity.\n",
        "  - **Data Issues**: Poor preprocessing or mislabeled data.\n",
        "\n",
        "- **L1/L2 regularization** affects networks by:\n",
        "  - **L1 Regularization**: Encourages sparsity by driving some weights to zero.\n",
        "  - **L2 Regularization**: Reduces large weights to improve generalization.\n",
        "\n",
        "- Advantages of **deep learning** over traditional methods:\n",
        "  - **Feature Learning**: Automatically extracts hierarchical features.\n",
        "  - **Flexibility**: Handles complex, non-linear relationships.\n",
        "  - **Scalability**: Performs well with large-scale data.\n",
        "  - **State-of-the-Art Results**: Excels in domains like computer vision and NLP."
      ],
      "metadata": {
        "id": "DQ1D06-WqH6q"
      },
      "id": "DQ1D06-WqH6q"
    },
    {
      "cell_type": "markdown",
      "id": "85175b87",
      "metadata": {
        "editable": true,
        "id": "85175b87"
      },
      "source": [
        "## Exercise 3: Decision trees and ensemble methods\n",
        "\n",
        "1. Mention some pros and cons when using decision trees\n",
        "\n",
        "2. How do we grow a tree? And which are the main parameters?\n",
        "\n",
        "3. Mention some of the benefits with using ensemble methods (like bagging, random forests and boosting methods)?\n",
        "\n",
        "4. Why would you prefer a random forest instead of using Bagging to grow a forest?\n",
        "\n",
        "5. What is the basic philosophy behind boosting methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0Kpo2hhSqIl5"
      },
      "id": "0Kpo2hhSqIl5"
    },
    {
      "cell_type": "markdown",
      "id": "fbfdfe68",
      "metadata": {
        "editable": true,
        "id": "fbfdfe68"
      },
      "source": [
        "## Exercise 4: Optimization part\n",
        "\n",
        "1. Which is the basic mathematical root-finding method behind essentially all gradient descent approaches(stochastic and non-stochastic)?\n",
        "\n",
        "2. And why don't we use it? Or stated differently, why do we introduce the learning rate as a parameter?\n",
        "\n",
        "3. What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an optimizer for the learning rate?\n",
        "\n",
        "4. Why should we use stochastic gradient descent instead of plain gradient descent?\n",
        "\n",
        "5. Which parameters would you need to tune when use a stochastic gradient descent approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "\n",
        "- **Pros and cons of decision trees**:\n",
        "  - **Pros**:\n",
        "    - Easy to understand and interpret.\n",
        "    - Handles both numerical and categorical data.\n",
        "    - Non-parametric, requiring no assumptions about data distribution.\n",
        "  - **Cons**:\n",
        "    - Prone to overfitting, especially with deep trees.\n",
        "    - Sensitive to small changes in the data (high variance).\n",
        "    - Can struggle with imbalanced datasets.\n",
        "\n",
        "- **How do we grow a tree, and what are the main parameters?**:\n",
        "  - A tree is grown by recursively splitting the dataset based on the feature that maximizes a splitting criterion (e.g., **Gini impurity**, **entropy**, or **variance reduction**).\n",
        "  - Main parameters include:\n",
        "    - **Max Depth**: Limits the depth of the tree to control overfitting.\n",
        "    - **Min Samples Split**: Minimum samples required to split a node.\n",
        "    - **Min Samples Leaf**: Minimum samples required in a leaf node.\n",
        "    - **Max Features**: Number of features considered for the best split.\n",
        "\n",
        "- **Benefits of using ensemble methods**:\n",
        "  - **Bagging** (e.g., Random Forests):\n",
        "    - Reduces variance by averaging predictions from multiple models.\n",
        "    - Improves stability and robustness.\n",
        "  - **Boosting** (e.g., Gradient Boosting, AdaBoost):\n",
        "    - Focuses on reducing bias by combining weak learners iteratively.\n",
        "    - Can achieve high accuracy with complex datasets.\n",
        "  - Both methods help mitigate overfitting compared to individual models.\n",
        "\n",
        "- **Why prefer Random Forests over Bagging?**:\n",
        "  - Random Forests introduce **feature randomness** by selecting a random subset of features for each split. This reduces correlation between individual trees, leading to improved generalization compared to Bagging, which uses all features.\n",
        "\n",
        "- **Basic philosophy behind boosting methods**:\n",
        "  - Boosting focuses on **sequentially building models** where each subsequent model corrects the errors of its predecessors. It assigns higher weights to misclassified samples, ensuring the next model pays more attention to them. The final prediction is an aggregated result of all the models, often weighted based on their performance.\n"
      ],
      "metadata": {
        "id": "ZAYKt2uzqI_N"
      },
      "id": "ZAYKt2uzqI_N"
    },
    {
      "cell_type": "markdown",
      "id": "92fc1b0c",
      "metadata": {
        "editable": true,
        "id": "92fc1b0c"
      },
      "source": [
        "### Exercise 5: Analysis of results\n",
        "1. How do you assess overfitting and underfitting?\n",
        "\n",
        "2. Why do we divide the data in test and train and/or eventually validation sets?\n",
        "\n",
        "3. Why would you use resampling methods in the data analysis? Mention some widely popular resampling methods."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "\n",
        "- **How do you assess overfitting and underfitting?**:\n",
        "  - **Overfitting**: The model performs well on the training set but poorly on the test/validation set. Indicators:\n",
        "    - Low training error but high test error.\n",
        "    - Significant difference between training and test performance metrics.\n",
        "  - **Underfitting**: The model performs poorly on both the training and test sets. Indicators:\n",
        "    - High error on the training set.\n",
        "    - Inability to capture the complexity of the data.\n",
        "\n",
        "- **Why do we divide the data into test, train, and/or validation sets?**:\n",
        "  - **Training Set**: Used to train the model.\n",
        "  - **Validation Set**: Used to tune hyperparameters and select the best model configuration without biasing the final evaluation.\n",
        "  - **Test Set**: Used to evaluate the model's performance on unseen data, providing an unbiased estimate of its generalization capability.\n",
        "  - Dividing data ensures that the model is not evaluated on the same data it was trained on, which helps detect overfitting and underfitting.\n",
        "\n",
        "- **Why use resampling methods in data analysis? Mention some popular methods**:\n",
        "  - Resampling methods are used to:\n",
        "    - Estimate the stability and reliability of model performance.\n",
        "    - Make better use of limited data by creating multiple training/testing splits.\n",
        "    - Provide robust evaluation metrics.\n",
        "  - Popular resampling methods:\n",
        "    - **Cross-Validation**: Divides data into \\( k \\)-folds, using \\( k-1 \\) folds for training and the remaining fold for testing, iteratively.\n",
        "    - **Bootstrapping**: Samples data with replacement to create multiple datasets for training/testing, useful for estimating uncertainty.\n",
        "    - **Leave-One-Out Cross-Validation (LOOCV)**: Uses all data points except one for training and the remaining point for testing, repeated for all points.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGWpLyCqqJYa"
      },
      "id": "uGWpLyCqqJYa"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}